<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Projects — Yanqing Wu</title>

  <link rel="stylesheet" href="static/css/main.css?v=desert1">

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-89371373-1', 'auto');
    ga('send', 'pageview');
  </script>
</head>
<body>

  <nav class="site-nav" role="navigation" aria-label="Primary">
    <ul>
      <li><a href="index.html">Home</a></li>
      <li><a href="projects.html" aria-current="page">Projects</a></li>
      <li><a href="https://blog.yanqingwu.com/" target="_blank" rel="noopener">(old) Blog</a></li>
    </ul>
  </nav>
  
  
  <section id="projects">
    <div class="project">
      <div class="project-content">
        <p><a href="#robotics-gui"> --> &#x1F916; <strong>Click to jump to Robotics + Simulation and GUI Projects section</strong> &#x1F916;  <-- </a></p>
        <!-- ML / DL / NLP -->
        <h4>Machine Learning (ML) / Deep Learning (DL) / Natural Language Processing (NLP) </h4>
        <ul class="project-list">
          <li>
            <strong><a href="https://github.com/pwyq/ECE356/tree/master" target="_blank" rel="noopener">ML/NLP: 2020 U.S. Election Twitter Sentiment Analysis</a></strong> — Collected over 3M tweets using the Twitter API, performed data cleaning and preprocessing, and applied TF-IDF vectorization with Logistic Regression and Naive Bayes classifiers to predict sentiment (positive, negative, neutral) toward presidential candidates. Analyzed trends over time and visualized sentiment distributions using matplotlib and seaborn.
            <em>Tech:</em> Python, scikit-learn, pandas, NumPy, matplotlib, seaborn, Twitter API, NLP, sentiment classification
          </li>
          <li>
            <strong><a href="https://github.com/pwyq/cse2525-data-mining/blob/master/challenges/Yanqing_Wu_Code.py" target="_blank" rel="noopener">Netflix Challenge — Movie Rating Prediction</a> <a href="https://github.com/pwyq/cse2525-data-mining/blob/master/challenges/reports/main.pdf" target="_blank" rel="noopener">[report]</a></strong> — Built an <em>Item-Item Collaborative Filtering</em> system using Pearson correlation (centered cosine similarity), achieving RMSE 0.84991 on Kaggle <strong>(rank 1/250+)</strong>. Tuned nearest neighbors <code>N</code>, applied rating capping, and tested dynamic <code>N</code> selection; added cold-start handling via local deviation adjustment; evaluated on held-out datasets via <em>Kaggle API</em>.
            <em>Tech:</em> Python, NumPy, Pandas, Kaggle API.
          </li>
          <li>
            <strong>ML/DL: Prediction of Gait Freezing in Parkinson’s Disease</strong> — Built machine learning models (LSTM and CNN) to detect and predict gait freezing episodes from wearable sensor data in PD patients. Achieved competitive accuracy in classification tasks and evaluated models with confusion matrices and ROC curves.
            <em>Tech:</em> MATLAB, Deep Learning Toolbox.
          </li>
          <li>
            <strong><a href="https://github.com/pwyq/stock-trading" target="_blank" rel="noopener">ML/NLP: Stock Price Prediction via News Sentiment</a></strong> — WIP: Building a machine learning model to forecast stock trends based on financial news sentiment, combining text analytics with historical market data.
            <em>Tech:</em> Python, Pandas, scikit-learn, NLP, sentiment analysis, time series forecasting
          </li>
        </ul>

        <!-- Computer Graphics -->
        <h4>Computer Graphics (CG)</h4>
        <ul class="project-list">
          <li>
            <strong><a href="https://github.com/pwyq/cg_project" target="_blank" rel="noopener">CG: Ray Tracer</a></strong> — Implemented a ray tracing engine from scratch, supporting recursive reflections, refractions, Phong shading, and multiple light sources.
            <em>Tech:</em> C++, OpenGL (for preview), image output to PPM.
          </li>
        </ul>

        <!-- Computer Vision / Robotics -->
        <h4>Computer Vision (CV) / Robotics</h4>
        <ul class="project-list">
          <li>
            <strong>ML/CV: Semantic Segmentation</strong> — Developed computer vision pipeline to classify each pixel into semantic categories, enabling scene understanding for downstream robotics tasks. Benchmarked against BiSeNet.
            <em>Tech:</em> Python, OpenCV, PyTorch
          </li>
          <li>
            <strong><a href="https://github.com/pwyq/UWRoboticsTeam/tree/master/MarsRover2017" target="_blank" rel="noopener">CV: Object Contour Detection & Following</a></strong> — Implemented contour detection to identify and track object boundaries in real time, integrating with motion control for robotic following.
            <em>Tech:</em> Python, OpenCV, edge detection, contour extraction
          </li>
          <li>
            <strong>Robotics: Line-Following Music Robot</strong> — A line-following robot that converts varying grayscale levels into corresponding sound frequencies, integrating motion control with auditory feedback.
            <em>Tech:</em> Arduino programming, hardware design (MOSFET, motors), sensor integration, algorithm design
          </li>
        </ul>

        <!-- Hardware / Embedded -->
        <h4>Hardware / Embedded Systems</h4>
        <ul class="project-list">
          <li>
            <strong><a href="https://github.com/pwyq/FPGA-Music-Player/blob/master/music_player.c" target="_blank" rel="noopener">Verilog: FPGA Music Player</a></strong> — Designed and implemented a music playback system on an Altera DE2 FPGA board, supporting WAV audio decoding, SD card file I/O, real-time playback control, and synchronized LED visual effects. Implemented hardware control for push buttons and switches, along with modular Verilog design for audio output via the audio codec chip.
            <em>Tech:</em> Verilog, C, Quartus Prime, Altera DE2, Audio Codec, SD Card I/O.
          </li>
          <li>
            <strong><a href="https://github.com/pwyq/BME351" target="_blank" rel="noopener">Verilog: Kirsch Edge Detection</a></strong> — Implemented the Kirsch operator in Verilog for 8-directional edge detection on grayscale images, optimized for high-throughput processing in FPGA-based systems.
            <em>Tech:</em> Verilog, FPGA design, hardware optimization, image processing algorithms
          </li>
        </ul>

        <!-- Apps / Software Tools -->
        <h4>Apps & Software Tools</h4>
        <ul class="project-list">
          <li>
            <strong><a href="https://github.com/pwyq/2048-course-project/tree/master/source%20code" target="_blank" rel="noopener">App Development: 2048 Game (Android)</a></strong> — Implemented both <em>touch</em> swipes and <em>sensor</em> control: shake/tilt in each direction to trigger moves, using the device accelerometer with a high-pass filter to detect directional impulses. Smooth animations and score/undo included.
            <em>Tech:</em> Java (Android Studio), SensorManager (Accelerometer), View animations.
          </li>
          <li>
            <strong>Matrix Calculator (CLI)</strong> — Implemented a matrix computation suite (for integers only) with file/terminal input, supporting arithmetic, linear algebra solvers, and property analysis (rank, determinant, transpose, power), with Python helpers for matrix generation and result validation.
            <em>Tech:</em> C (core), Python (generation & validation).
          </li>
        </ul>
      </div>
    </div>
  </section>

  <section id="projects">
    <h1 id="robotics-gui">Projects with Visual Demo (Robotics + Simulation, or GUI)</h1>
    <div class="project">
      <img src="static/img/Belludum-logo-blue-red.svg" alt="Belludum Logo" />
      <div class="project-content">
        <h3>Simulation-Driven Web Game (Stealth project; WIP)</h3>
        <p>
          A real-time, military-themed, multiplayer, semi-automation/incremental webgame.
          Designed for maintainability, expandability, and performance — built entirely from scratch with no templates.
        </p>
        <p>
          I designed and implemented both front-end and back-end systems, including user authentication,
          language localization, and gameplay logic — all while self-learning full-stack development and Vue.
        </p>
        <ul class="tools">
          <li><strong>Vue 3 + Vite</strong>: Reactive front-end framework with lightning-fast build system</li>
          <li><strong>TypeScript</strong>: Static typing for code robustness and maintainability</li>
          <li><strong>Pinia</strong>: Centralized state management for UI/data sync</li>
          <li><strong>BEM CSS</strong>: Structured CSS for consistent styling</li>
          <li><strong>UI/UX design</strong>: Custom layout for dashboard and card interface</li>

          <li><strong>Node.js</strong>: Server logic and API endpoints</li>
          <li><strong>MongoDB Atlas</strong>: Cloud-hosted database for users, teams, and game state</li>
          <li><strong>JWT Auth</strong>: Secure user authentication</li>
          <li><strong>CORS config</strong>: Safe cross-origin API calls</li>

          <li><strong>WebSocket</strong>: Real-time multiplayer chat and game state updates</li>
          <li><strong>Axios</strong>: Standard API requests</li>
          <li><strong>Browser storage</strong>: Persistent user settings and localization</li>

          <li><strong>Jest</strong>: Unit tests</li>
          <li><strong>GitHub Actions</strong>: CI/CD</li>

          <li><strong>Core game design</strong>: Real-time sim engine, progress bars, idle/active hybrid</li>
          <li><strong>Anti-hacking logic</strong>: Defensive coding and validation</li>
          <li><strong>User system</strong>: Account, login, progress tracking</li>
          <li><strong>Resource system</strong>: Mining, construction, conversion, storage</li>
          <li><strong>Dashboard</strong>: Teams, missions, reports</li>

          <li><strong>Design thinking</strong>: Future-proofing, reusability, coherence</li>
          <li><strong>vibe-coding</strong>: Accelerated development using code suggestions</li>
        </ul>
      </div>
    </div>

    <div class="project">
      <img src="static/img/FillGame.png" alt="Fill-Game screenshot" />
      <div class="project-content">
        <h3>Fill-Game, with AI Opponents <a href="https://github.com/pwyq/Fill-Game" target="_blank" rel="noopener">[GitHub]</a> <a href="https://drive.google.com/file/d/1PRzAiC2bDOrCQAVQzM4fdMjzkgKp-8Ab/view?usp=drive_link" target="_blank" rel="noopener">[report]</a></h3>
        <p>
          A two-player, adversarial version of Fillomino—built with heuristic search algorithms to support both PvE (Human vs. AI) and PvP (Human vs. Human) play.
        </p>
        <p>
          Implemented in modern C++ (with a Qt GUI), and featuring a variety of search strategies—Depth-First Proof Number Search (DFPN), Proof Number Search (PNS), Minimax & Negamax (each with alpha-beta pruning and transposition tables), plus Monte Carlo Tree Search (MCTS).
        </p>
        <ul class="tools">
          <li><strong>C++ 20</strong></li>
          <li><strong>Qt</strong> (cross-platform GUI)</li>
          <li><strong>DFPN, PNS</strong>: Strategic search planning</li>
          <li><strong>Minimax / Negamax</strong> w/ alpha-beta pruning & transposition tables</li>
          <li><strong>MCTS</strong>: Probabilistic, long-horizon planning</li>
          <li><strong>Cross-machine & local play</strong></li>
          <li><strong>Unit tests</strong> & performance tuning</li>
          <li><strong>Profiler</strong>: Valgrind, KCachegrind</li>
        </ul>
      </div>
    </div>

    <div class="project">
      <img src="static/img/viwistar.gif" alt="IRB2600 welding task simulation" />
      <div class="project-content">
        <h3>Reinforcement Learning: IRB2600 Welding Task Simulation</h3>
        <p>
          A simulation of an ABB IRB2600 robot executing a welding operation in a virtual environment.
          The project focused on task-specific motion planning, welding path generation, and fine-tuning
          joint trajectories for smooth and accurate operation.
        </p>
        <p>
          Combined industrial simulation software with scripting to automate and visualize welding processes before deployment. I also
          benchmarked reinforcement learning approaches — <strong>Proximal Policy Optimization (PPO)</strong> and
          <strong>Soft Actor-Critic (SAC)</strong> — to evaluate their performance in learning efficient welding paths.
        </p>
        <ul class="tools">
          <li><strong>RL Algorithms</strong>: PPO, SAC</li>
          <li><strong>Python</strong>: Welding path scripting, RL benchmarking</li>
          <li><strong>Lua</strong>: Custom motion control in CoppeliaSim (V-REP)</li>
          <li><strong>CoppeliaSim (V-REP)</strong>: Kinematics and task validation</li>
          <li><strong>Ubuntu</strong>: Development and runtime environment</li>
        </ul>
      </div>
    </div>

    <div class="project">
      <img src="static/img/IRB2600_moving_in_RobotStudio.gif" alt="IRB2600 moving in RobotStudio" />
      <div class="project-content">
        <h3>Dual Simulation: IRB2600 with ABB RobotStudio and CoppeliaSim (V-rep)</h3>
        <p>
          A simulation project demonstrating motion planning and execution for the ABB IRB2600 industrial robot arm.
          Developed using a mix of scripting and simulation tools to bridge virtual prototyping with real-world robotics workflows.
        </p>
        <p>
          The work included creating robot motion sequences, integrating with ABB RobotStudio for visualization, and
          using CoppeliaSim (formerly V-REP) for simulation and control logic testing on Ubuntu.
        </p>
        <ul class="tools">
          <li><strong>Python</strong>: Control scripts and data handling</li>
          <li><strong>Lua</strong>: Embedded scripting for CoppeliaSim (V-REP)</li>
          <li><strong>ABB RobotStudio</strong>: Industrial robot simulation and programming</li>
          <li><strong>CoppeliaSim (V-REP)</strong>: Physics-based robot simulation</li>
          <li><strong>Ubuntu</strong>: Development and runtime environment</li>
        </ul>
      </div>
    </div>
    
    <div class="project">
      <video controls autoplay muted loop playsinline>
        <source src="static/img/OMPL_LazyPRM_Obstacle_Avoidance_demo.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
      <div class="project-content">
        <h3>OMPL LazyPRM Obstacle Avoidance</h3>
        <p>
          A motion planning demo using the <strong>Open Motion Planning Library (OMPL)</strong> with the LazyPRM algorithm
          to navigate around obstacles. Demonstrates efficient graph-based planning for environments with complex geometry.
        </p>
        <ul class="tools">
          <li><strong>OMPL</strong>: LazyPRM motion planning</li>
          <li><strong>Python</strong>: Planning orchestration and visualization</li>
          <li><strong>Ubuntu</strong>: Development and runtime environment</li>
        </ul>
      </div>
    </div>

    <div class="project">
      <video controls autoplay muted loop playsinline>
        <source src="static/img/husky_phase1.mp4" type="video/mp4">
      </video>

      <div class="project-content">
        <h3>Husky UGV — Teleop & Camera Perception (Gazebo + RViz)</h3>
        <p>
          A simulation and control stack for Clearpath’s Husky UGV: keyboard teleoperation in Gazebo with live camera
          streams visualized in RViz. Includes topic wiring, launch files, and a minimal perception pipeline for
          testing navigation behaviors in a sandboxed environment.
        </p>
        <ul class="tools">
          <li><strong>ORB_SLAM3</strong>: localization and mapping</li>
          <li><strong>ROS</strong>: node graph, topics, launch files</li>
          <li><strong>Gazebo</strong>: physics simulation, Husky model/plug-ins</li>
          <li><strong>RViz</strong>: visualization (camera/image topics, TF, robot model)</li>
          <li><strong>teleop_twist_keyboard</strong>: WASD keyboard control</li>
          <li><strong>Camera input</strong>: Gazebo camera / USB cam via <code>image_transport</code></li>
          <li><strong>Ubuntu</strong>: development & runtime</li>
          <li><strong>rosbag</strong>: data capture & replay (optional)</li>
          <li><strong>URDF/Xacro</strong>: robot model tweaks (optional)</li>
        </ul>
      </div>
    </div>

    <div class="project">
      <div class="video-wrapper">
        <iframe
          src="https://www.youtube.com/embed/1bRu2PWA9b8"
          title="BobaTron — Bubble Tea Self-Serving Machine"
          frameborder="0"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
          allowfullscreen>
        </iframe>
      </div>

      <div class="project-content">
        <h3>BobaTron — Bubble Tea Self-Serving Machine <a href="https://drive.google.com/file/d/19sXE8ziUkc5zCtOZvRPpPo5vq2iFzxu7/view?usp=drive_link">[report]</a></h3>
        <p>
          Fully automated, tablet-operated bubble tea machine integrating dispensing, assembly/delivery, power,
          micro-controller, and user interaction subsystems. Built to cut wait time and ensure taste consistency,
          with precise feedback-controlled dispensing and contact-less mixing.
        </p>
        <p>
          Highlights: load-cell feedback loop with pumps & Archimedes screw for accurate dosing; orbital shaker
          for hygienic mixing; tablet UI with Wi-Fi control; multi-Arduino network over I²C. The prototype meets
          key specs and produces a drink in ~70 seconds; the milk module actively cools from ~9.5 °C to ~3 °C
          in ~25 minutes.
        </p>
        <ul class="tools">
          <li><strong>Control</strong>: Multi-Arduino network (I²C), ESP32 Wi-Fi server, tablet UI</li>
          <li><strong>Dispensing</strong>: Diaphragm & peristaltic pumps, Archimedes screw, load-cell feedback</li>
          <li><strong>Assembly/Mixing</strong>: Conveyor & cup dispenser, orbital shaker (contact-less)</li>
          <li><strong>Power/Thermal</strong>: Regulated DC rails, Peltier cooling with temperature feedback</li>
          <li><strong>Performance</strong>: ≈70 sec/drink;</li>
          <li><strong>Cost</strong>: Prototype ≈ C$1,500 (under C$4,000 target)</li>
        </ul>
      </div>
    </div>
    <div class="project">
      <img src="static/img/wgviewer.png" alt="Warship Girls Viewer" />

      <div class="project-content">
        <h3 class="project-title">
          WGViewer — Game Automation & GUI Tool
          <a href="https://github.com/WarshipGirls/WGViewer" target="_blank" rel="noopener">[GitHub]</a>
          <a href="https://github.com/WarshipGirls/WGViewer/tree/master/screenshots" target="_blank" rel="noopener">
            [More Screenshots]
          </a>
        </h3>
        <p class="project-description">
          Developed a cross-platform GUI tool for <em>Warship Girls</em>, a popular mobile game in Asia, enabling 
          advanced automation and in-depth game data management. 
          The project reverse-engineered and integrated with the game’s API to provide full automated gameplay, 
          equipment management, and resource optimization features. 
          Achieved <strong>50,000+ downloads</strong> from Chinese gaming forums.
        </p>
        <ul class="tools">
          <li><strong>GUI Development</strong>: Built an intuitive, cross-platform interface using <em>PyQt</em>.</li>
          <li><strong>Game API Reverse Engineering</strong>: Automated gameplay, resource/equipment tracking, and event handling.</li>
          <li><strong>Automation Engine</strong>: Designed routines for fully unattended daily tasks, battles, and upgrades.</li>
          <li><strong>Version Management</strong>: Created modular architecture to quickly adapt to frequent game updates.</li>
          <li><strong>CI/CD</strong>: Set up GitHub Actions for automated builds and releases.</li>
          <li><strong>Asset Organization</strong>: Structured resources for images, configs, and logs for maintainability.</li>
          <li><strong>Performance Optimization</strong>: Improved memory and network efficiency for long-running sessions.</li>
        </ul>
      </div>
    </div>


    <div class="project">
      <img src="static/img/mhnow-automation.png" alt="Monster Hunter NOW automation" />
      <div class="project-content">
        <h3>Mobile Game Automation & Reverse Engineering
          <a href="https://github.com/pwyq/MHNow_Farm" target="_blank" rel="noopener">[GitHub]</a>
        </h3>
        <p>
          A personal deep-dive into automation and memory-level reverse engineering of a real-time AR ARPG,
          <em>Monster Hunter NOW</em> (from Pokemon Go company). Explored mobile system behavior, game memory, and scripting logic.
        </p>
        <ul class="tools">
          <li><strong>OpenCV + Vysor:</strong> Vision-based automation of hunting flows</li>
          <li><strong>Python Scripting:</strong> Real-time control logic for timing, items, decisions</li>
          <li><strong>Rooted Android:</strong> Elevated permissions for GPS spoofing</li>
          <li><strong>GameGuardian:</strong> Address-based memory inspection for weapon stats</li>
          <li><strong>APK reverse engineering:</strong> Decompiled client for hidden assets/keys (educational)</li>
        </ul>
        <p>
          The system could <strong>hunt all monsters automatically</strong>. I was among the earliest to reach
          <strong>max level and a complete weapon collection</strong> via scripting and reverse engineering.
        </p>
      </div>
    </div>


  </section>
</body>
</html>
